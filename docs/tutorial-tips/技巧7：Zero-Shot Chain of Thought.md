---
sidebar_position: 7
---
<head>
  <script defer="defer" src="https://embed.trydyno.com/embedder.js"></script>
  <link href="https://embed.trydyno.com/embedder.css" rel="stylesheet" />
</head>


基於上述的第三點缺點，研究人員就找到了一個叫 Chain of Thought 的技巧。

這個技巧使用起來非常簡單，只需要在問題的結尾裡放一句 `Let‘s think step by step` （讓我們一步步地思考），模型輸出的答案會更加準確。

這個技巧來自於 Kojima 等人 2022 年的論文 [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)。在論文裡提到，當我們向模型提一個邏輯推理問題時，模型返回了一個錯誤的答案，但如果我們在問題最後加入 `Let‘s think step by step` 這句話之後，模型就生成了正確的答案：

![ZeroShotChainOfThought001.png](./assets/ZeroShotChainOfThought001.png)

論文裡有講到原因，感興趣的朋友可以去看看，我簡單解釋下為什麼（🆘 如果你有更好的解釋，不妨反饋給我）：

1. 首先各位要清楚像 ChatGPT 這類產品，它是一個統計語言模型，本質上是基於過去看到過的所有資料，用統計學意義上的預測結果進行下一步的輸出（這也就是為什麼你在使用 ChatGPT 的時候，它的答案是一個字一個字地吐出來，而不是直接給你的原因，因為答案是一個字一個字算出來的）。
2. 當它拿到的資料裡有邏輯，它就會透過統計學的方法將這些邏輯找出來，並將這些邏輯呈現給你，讓你感覺到它的回答很有邏輯。
3. 在計算的過程中，模型會進行很多假設運算（不過暫時不知道它是怎麼算的）。比如解決某個問題是從 A 到 B 再到 C，中間有很多假設。
4. 它第一次算出來的答案錯誤的原因，只是因為它在中間跳過了一些步驟（B）。而讓模型一步步地思考，則有助於其按照完整的邏輯鏈（A > B > C）去運算，而不會跳過某些假設，最後算出正確的答案。

按照論文裡的解釋，零樣本思維鏈涉及兩個自動完成結果，左側氣泡表示基於提示輸出的第一次的結果，右側氣泡表示其收到了第一次結果後，將最開始的提示一起拿去運算，最後得出了正確的答案：

![ZeroShotChainOfThought002.png](./assets/ZeroShotChainOfThought002.png)

這個技巧，用於解複雜問題有用外，還適合產生一些連貫主題的恩榮，比如寫長篇文章、電影劇本等。

但需要注意其缺點，連貫不代表，它就一定不會算錯，如果其中某一步驟算錯了，錯誤會因為邏輯鏈，逐步將錯誤積累，導致產生的文字可能出現與預期不符的內容。

另外，根據 Wei 等人在 [2022 年的論文](https://arxiv.org/pdf/2201.11903.pdf)表明，還有它僅在大於等於 100B 引數的模型中使用才會有效。如果你使用的是小樣本模型，這個方法不會生效。

