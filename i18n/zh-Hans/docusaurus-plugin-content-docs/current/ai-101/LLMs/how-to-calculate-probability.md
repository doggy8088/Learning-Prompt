---
sidebar_position: 2
---

# 如何計算機率？

既然是數學模型，那應該如何計算呢？

最簡單的方法，當然就是用統計學的方法去計算了，簡單說來，就是靠輸入的上下文進行統計，計算出後續詞語的機率，比如「你吃了晚飯了嗎」，「你吃了」後面按照機率，名詞如「飯」或「晚飯」等機率更高，而不太可能是動詞，如「睡」「睡覺」。

這是語言模型的第一階段，模型也被稱為是統計語言模型（Statistical Language Model，SLM），其基本思想是基於馬爾可夫假設建立詞語測模型，根據最近的上下文預測下一個詞。

後續語言模型的發展又迭代了三個版本。

第二階段是神經網絡語言模型（Neural Language Model，NLM），是一個用神經網路來訓練模型，學習單詞之間的關聯性和機率關係。它能夠利用大量的資料進行深度學習，從而捕捉到詞彙之間更加複雜的關係。NLM 模型採用的是分層的結構，把輸入的文字資料空間投射到高維的語義空間中並進行學習。透過不斷地更新神經網路模型引數，NLM 的神經網路逐漸學會了文字資料的語義並能夠生成連貫自然、語義準確的文字。

與前面提到的 SLM 相比，由於深度神經網路的學習能力更強，NLM 在學習語言模型時具有更好的泛化能力和適應性。比如能生成更長的文字等。但 NLM 相對來說也比較依賴更大的資料集，並且需要花很多人力在資料標註上。

第三階段是預訓練語言模型（Pre-trained Language Model，PLM），是一種使用大量文字資料來訓練的自然語言處理模型。相對 NLM 來說，PLM 使用無監督學習方法，因此不需要先標註資料或註明文字型別等訊息。各位可能聽過的 Transformer 架構就是一種預訓練語言模型。

第四階段是大預言模型（Large Language Model），你可以將現在的 LLM 理解為一個訓練資料特別大的 PLM，比如 GPT-2 只有 1.5B 引數，GPT-3 則到了驚人 175B，儘管 LLM 只是拓展了模型的大小，但這些大尺寸的預訓練語言模型表現出了與較小的預訓練語言模型不同的行為，並且在解決一些複雜任務上展現了驚人的能力（俗稱湧現能力，注意這個湧現能力目前還存在爭議），所以學術界為這些大型預訓練語言模型命名為大語言模型 LLM。

上面這四個階段可能比較難理解，你可以簡單理解：

1. 語言模型本質上都是在計算自然語言每個句子的機率的數學模型。當你輸入一個問題給 AI 時，AI 就是用機率算出它的回答。
2. 另外，當今的語言模型，並不是一個問題對一個答案，實際上是一個問題，多個答案，然後根據答案的機率進行排序，最後回傳一個最可能的答案。

以上兩個認知非常重要。
